{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aaa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters and Configuration ---\n",
    "\n",
    "# Number of training epochs\n",
    "epoch_num = 500\n",
    "# Number of samples in each batch\n",
    "batch_size = 40\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Directory containing the training and validation data subfolders\n",
    "train_data_dir = \"./train-val-independent/\"\n",
    "\n",
    "# File paths for the generated CSV files listing the training and validation data\n",
    "train_fn = \"./csv_lists_indep/trainData.csv\"\n",
    "eval_fn = \"./csv_lists_indep/valiData.csv\"\n",
    "\n",
    "# A string identifier for this specific training run, used in output folder names\n",
    "str_id = \"_msplus_v2_independent\"\n",
    "\n",
    "# Directories to save model checkpoints and TensorBoard logs\n",
    "output_dir = f\"./results{str_id}/checkpoints\"\n",
    "log_dir = f\"./results{str_id}/logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Library Imports ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cddf5-2c51-42b0-bde8-9de44d8f2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CSV Generation for Data Loading ---\n",
    "\n",
    "# Create the directories for CSV files if they don't already exist\n",
    "os.makedirs(os.path.dirname(train_fn), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(eval_fn), exist_ok=True)\n",
    "\n",
    "def generate_csv(folder_path, output_csv):\n",
    "    \"\"\"\n",
    "    Generates a CSV file from a directory structure.\n",
    "    Assumes that subdirectories in folder_path are class labels (e.g., species names).\n",
    "    The CSV will have columns: Fname, Genera, Species.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): The path to the data directory (e.g., \"./data/train/\").\n",
    "        output_csv (str): The path where the output CSV file will be saved.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary mapping each species label to a unique integer ID.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    label_list = set()\n",
    "\n",
    "    # Iterate through the subdirectories (each representing a species)\n",
    "    for species in os.listdir(folder_path):\n",
    "        label_list.add(species)\n",
    "        species_path = os.path.join(folder_path, species)\n",
    "        \n",
    "        if os.path.isdir(species_path):\n",
    "            # Iterate through the audio files in the species directory\n",
    "            for fname in os.listdir(species_path):\n",
    "                # Ensure it's a file before adding\n",
    "                if os.path.isfile(os.path.join(species_path, fname)):\n",
    "                    data.append({\n",
    "                        \"Fname\": os.path.join(species_path, fname).replace(\"\\\\\", \"/\"),\n",
    "                        \"Genera\": \"\",  # Genera is not used in this script, kept for compatibility\n",
    "                        \"Species\": species\n",
    "                    })\n",
    "\n",
    "    # Write the collected data to a CSV file\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"Fname\", \"Genera\", \"Species\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "        \n",
    "    # Return a mapping of class names to integer indices\n",
    "    return {key: idx for idx, key in enumerate(label_list)}\n",
    "\n",
    "# Generate the CSV for the training dataset\n",
    "label_list = generate_csv(train_data_dir + \"train/\", train_fn)\n",
    "print(f\"Train CSV created: {train_fn}\")\n",
    "\n",
    "# Generate the CSV for the validation dataset\n",
    "generate_csv(train_data_dir + \"validation/\", eval_fn)\n",
    "print(f\"Validation CSV created: {eval_fn}\")\n",
    "\n",
    "print(f\"Generated label map: {label_list}\")\n",
    "# Set the number of classes based on the discovered labels\n",
    "cls_num = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b06a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Definition (MosquitoSongPlus) ---\n",
    "\n",
    "class MosquitoSongPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    A 1D Convolutional Neural Network for audio classification, inspired by MosquitoSong.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=16000, num_classes=8):\n",
    "        super(MosquitoSongPlus, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=100, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=64, stride=4, padding=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=64, stride=3, padding=2)\n",
    "\n",
    "        # Max-pooling layer\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Dynamically calculate the input size for the first fully connected layer\n",
    "        fc_input_size = self.calculate_fc_input_size(input_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(fc_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def calculate_fc_input_size(self, input_size):\n",
    "        \"\"\"\n",
    "        Calculates the flattened feature size after all conv and pool layers.\n",
    "        This makes the model adaptable to different input audio lengths.\n",
    "        Formula for Conv1d output size: floor((L_in + 2*padding - kernel_size) / stride) + 1\n",
    "        Formula for MaxPool1d output size: floor((L_in - kernel_size) / stride) + 1\n",
    "        \"\"\"\n",
    "        size = input_size\n",
    "        size = (size + 2 * 2 - 100) // 4 + 1  # After conv1\n",
    "        size = (size - 3) // 3 + 1             # After pool1\n",
    "        size = (size + 2 * 2 - 64) // 4 + 1   # After conv2\n",
    "        size = (size - 3) // 3 + 1             # After pool2\n",
    "        size = (size + 2 * 2 - 64) // 3 + 1   # After conv3\n",
    "        size = (size - 3) // 3 + 1             # After pool3\n",
    "        # The final size is multiplied by the number of output channels from the last conv layer\n",
    "        return size * 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \"\"\"\n",
    "        # Input shape: (batch_size, 1, input_size)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten the features for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # We return raw logits because nn.CrossEntropyLoss applies softmax internally.\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MosquitoSongPlus(input_size=16000, num_classes=cls_num)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "# Load the CSV datasets using the Hugging Face `datasets` library\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": train_fn, \"test\": eval_fn})\n",
    "print(dataset)\n",
    "\n",
    "# Create label-to-id and id-to-label mappings\n",
    "label2id = label_list\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def preprocess_csv_function(examples):\n",
    "    \"\"\"Maps the species name (string) to an integer label.\"\"\"\n",
    "    return {\n",
    "        \"label\": label2id[examples[\"Species\"]],\n",
    "        \"audio_filepath\": examples[\"Fname\"]\n",
    "    }\n",
    "\n",
    "# Apply the initial CSV preprocessing\n",
    "csv_processed_dataset = dataset.map(preprocess_csv_function)\n",
    "print(csv_processed_dataset[\"train\"][0])\n",
    "\n",
    "train_dataset = csv_processed_dataset[\"train\"]\n",
    "validation_dataset = csv_processed_dataset[\"test\"]\n",
    "print(f\"Validation dataset info: {validation_dataset}\")\n",
    "print(f\"Class labels (id2label): {id2label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_function(examples, target_length=16000):\n",
    "    \"\"\"\n",
    "    Loads, normalizes, and pads/truncates an audio file to a target length.\n",
    "    \"\"\"\n",
    "    audio_fn = examples['audio_filepath']\n",
    "    # Load audio at a 16kHz sample rate\n",
    "    audio, sr = librosa.load(audio_fn, sr=16000)\n",
    "    # Normalize the audio waveform\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    \n",
    "    # Pad or truncate the audio to ensure uniform length\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode=\"constant\")\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    return {\"input_values\": audio, \"labels\": int(examples[\"label\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a992f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the audio preprocessing function to both datasets\n",
    "encoded_train_dataset = train_dataset.map(preprocess_audio_function, remove_columns=[\"Fname\", \"Genera\", \"Species\"])\n",
    "encoded_validation_dataset = validation_dataset.map(preprocess_audio_function, remove_columns=[\"Fname\", \"Genera\", \"Species\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encoded_train_dataset[\"input_values\"][0])\n",
    "#encoded_train_dataset[\"input_values\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to batch data points.\n",
    "    It stacks input values into a single tensor and creates a tensor for labels.\n",
    "    \"\"\"\n",
    "    # Convert list of numpy arrays to a batch tensor of type float32\n",
    "    input_values = torch.stack([torch.tensor(item[\"input_values\"], dtype=torch.float32) for item in batch])\n",
    "    # Convert list of integer labels to a batch tensor of type long\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics, Training, and Evaluation Functions ---\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"Computes various classification metrics.\"\"\"\n",
    "    # Get the predicted class by finding the index of the max logit\n",
    "    predictions = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    # Use 'weighted' average for multiclass precision, recall, f1 to account for label imbalance\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"balanced_accuracy\": balanced_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        # Move data to the specified device (GPU or CPU)\n",
    "        # unsqueeze(1) adds the channel dimension: (batch, length) -> (batch, 1, length)\n",
    "        inputs = batch[\"input_values\"].unsqueeze(1).to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, eval_loader, criterion, device):\n",
    "    \"\"\"Performs evaluation on the validation set.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs = batch[\"input_values\"].unsqueeze(1).to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "    # Concatenate all batch predictions and labels\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Compute metrics over the entire validation set\n",
    "    metrics = compute_metrics(all_predictions, all_labels)\n",
    "    \n",
    "    return total_loss / len(eval_loader), metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Training Loop ---\n",
    "\n",
    "# Create output directories and initialize TensorBoard writer\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Set device, initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MosquitoSongPlus(input_size=16000, num_classes=len(label2id)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# AdamW is a variant of Adam with improved weight decay regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Create DataLoader instances for training and evaluation\n",
    "train_loader = DataLoader(encoded_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(encoded_validation_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Variables to track the best model\n",
    "best_balanced_acc = 0\n",
    "best_model_path = \"\"\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(epoch_num):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, metrics = evaluate(model, eval_loader, criterion, device)\n",
    "\n",
    "    # Log metrics to TensorBoard for visualization\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", metrics[\"accuracy\"], epoch)\n",
    "    writer.add_scalar(\"BalancedAccuracy/Validation\", metrics[\"balanced_accuracy\"], epoch)\n",
    "    writer.add_scalar(\"F1/Validation\", metrics[\"f1\"], epoch)\n",
    "\n",
    "    # Print epoch results to the console\n",
    "    print(f\"Epoch {epoch + 1}/{epoch_num}\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Metrics: {metrics}\")\n",
    "\n",
    "    # Save the model if it has the best balanced accuracy so far\n",
    "    if metrics[\"balanced_accuracy\"] > best_balanced_acc:\n",
    "        best_balanced_acc = metrics[\"balanced_accuracy\"]\n",
    "        best_model_path = os.path.join(output_dir, f\"best_model_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  ---> New best model saved to {best_model_path}\")\n",
    "\n",
    "# Save the final model after the last epoch\n",
    "last_model_path = os.path.join(output_dir, \"last_model.pt\")\n",
    "torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")\n",
    "print(f\"Best model was saved at: {best_model_path} with Balanced Accuracy: {best_balanced_acc:.4f}\")\n",
    "print(f\"Last model saved at: {last_model_path}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf611870-44b0-4f57-97a4-31e44ab29cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae972961-5ba2-406a-9a5d-0c41f8e17c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
