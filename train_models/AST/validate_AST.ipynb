{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir=\"../train-val_szunyog_hangok_osztalyozáshoz_3cls_25_02_14/validation/\"\n",
    "\n",
    "checkpoint_dir = \"./results_AST_multiclass_mosquito_25_01_28-full-3cls/checkpoint-2092\" # the best\n",
    "\n",
    "id2label = {0: 'Aedes_koreicus', 1: 'Ochlerotatus_geniculatus', 2: 'Aedes_albopictus'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "print(\"Checkpoint folder exists:\", os.path.exists(checkpoint_dir))\n",
    "print(\"Folder contents:\", os.listdir(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_dataset_from_folders(validation_dir, everyNth=1):\n",
    "    \"\"\"\n",
    "    Load data from folders and convert to Dataset format.\n",
    "\n",
    "    Args:\n",
    "        validation_dir (str): Path to the 'validation' folder.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A DatasetDict with 'validation' data.\n",
    "    \"\"\"\n",
    "    def get_audio_files_with_labels(directory):\n",
    "        data = []\n",
    "        for class_name in os.listdir(directory):  # Classes ('mosquito', 'not')\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\".wav\"):  # Only WAV files\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        data.append({\"file_path\": file_path, \"label\": class_name})\n",
    "        return data\n",
    "\n",
    "    # Load validation data\n",
    "    validation_data = get_audio_files_with_labels(validation_dir)\n",
    "\n",
    "    # Create Dataset\n",
    "\n",
    "    validation_dataset = Dataset.from_dict({\n",
    "        \"file_path\": [d[\"file_path\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0],\n",
    "        \"label\": [d[\"label\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0]\n",
    "    })\n",
    "\n",
    "    #return DatasetDict({\"validation\": validation_dataset})\n",
    "    return validation_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "validation_dataset = load_audio_dataset_from_folders(validation_dir)\n",
    "\n",
    "print(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_dir)\n",
    "model = AutoModelForAudioClassification.from_pretrained(checkpoint_dir)\n",
    "\n",
    "id2label=model.config.id2label\n",
    "label2id=model.config.label2id\n",
    "\n",
    "id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a39852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(audio_filename,model):\n",
    "    # Load audio file (with 16 kHz sampling rate)\n",
    "    audio, sr = librosa.load(audio_filename, sr=16000)\n",
    "\n",
    "    # Preprocess input\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n",
    "    \n",
    "    # Apply model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Class probabilities and predictions\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    #print(f\"Predicted class index: {predicted_class.item()}\")\n",
    "\n",
    "    # Map prediction to labels\n",
    "    id2label = model.config.id2label\n",
    "    #print(f\"Predicted class label: {id2label[predicted_class.item()]}\")\n",
    "    \n",
    "    return predicted_class.item(), id2label[predicted_class.item()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58178db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a52063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "audio_paths=[]\n",
    "\n",
    "# Iterate with a progress bar\n",
    "for example in tqdm(validation_dataset, desc=\"Processing validation set\"):\n",
    "#for example in tqdm(list(validation_dataset), desc=\"Processing validation set\"):\n",
    "    audio_filepath = example[\"file_path\"]\n",
    "    audio_paths.append(os.path.basename(audio_filepath))\n",
    "    true_label = example[\"label\"]\n",
    "    \n",
    "    # Prediction from the model\n",
    "    predicted_class, predicted_label = predict(audio_filepath, model)\n",
    "    \n",
    "    # Collect results\n",
    "    true_labels.append(true_label)\n",
    "    predicted_labels.append(predicted_class)\n",
    "\n",
    "\n",
    "res_csv=pd.DataFrame()\n",
    "res_csv['file']=audio_paths\n",
    "res_csv['true label']=true_labels\n",
    "res_csv['predicted class']=predicted_labels\n",
    "\n",
    "true_labels=[label2id[x] for x in true_labels]\n",
    "res_csv['true class']=true_labels\n",
    "res_csv.to_csv(\"eval_results_AST.csv\")\n",
    "\n",
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1beb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(4, 3)) # A figura méretét is növelhetjük, hogy jobban elférjenek a nagyobb betűk\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=id2label.values(), yticklabels=id2label.values(),\n",
    "            annot_kws={\"fontsize\": 14}) # Annotációk betűmérete\n",
    "plt.xlabel(\"Predicted Labels\", fontsize=16) # X tengely felirat betűmérete\n",
    "plt.ylabel(\"True Labels\", fontsize=16)     # Y tengely felirat betűmérete\n",
    "plt.title(\"Confusion Matrix\", fontsize=18) # Cím betűmérete\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b527dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for individual classes\n",
    "class_labels = list(id2label.keys())  # Class indices\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "# Binarize true labels for ROC-AUC calculation (one vs. all)\n",
    "true_labels_binarized = np.eye(len(class_labels))[true_labels]  # One-hot encoding\n",
    "predicted_labels_binarized = np.eye(len(class_labels))[predicted_labels]\n",
    "\n",
    "roc_auc_per_class = []\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels_binarized[:, i], predicted_labels_binarized[:, i])\n",
    "    except ValueError:  # If there is no positive sample\n",
    "        roc_auc = np.nan\n",
    "    roc_auc_per_class.append(roc_auc)\n",
    "\n",
    "# Print class-wise metrics\n",
    "print(\"\\nClass-wise metrics:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {id2label[label]}:\")\n",
    "    print(f\"  Precision: {precision_per_class[i]:.2f}\")\n",
    "    print(f\"  Recall: {recall_per_class[i]:.2f}\")\n",
    "    print(f\"  F1 Score: {f1_per_class[i]:.2f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_per_class[i]:.2f}\" if not np.isnan(roc_auc_per_class[i]) else \"  ROC-AUC: N/A\")\n",
    "\n",
    "# Visualization (class-wise metrics)\n",
    "metrics_df = {\n",
    "    \"Precision\": precision_per_class,\n",
    "    \"Recall\": recall_per_class,\n",
    "    \"F1 Score\": f1_per_class,\n",
    "    \"ROC-AUC\": roc_auc_per_class,\n",
    "}\n",
    "metrics_df = {id2label[label]: values for label, values in zip(class_labels, zip(*metrics_df.values()))}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for metric, values in metrics_df.items():\n",
    "    plt.plot(class_labels, values, marker='o', label=metric)\n",
    "\n",
    "plt.xticks(class_labels, [id2label[label] for label in class_labels], rotation=45)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Class-wise Metrics\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9545c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d29911",
   "metadata": {},
   "source": [
    "# top2 acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k accuracy\n",
    "\n",
    "def predict_top(audio_filename, model):\n",
    "    \n",
    "    # Load audio file (with 16 kHz sampling rate)\n",
    "    audio, sr = librosa.load(audio_filename, sr=16000)\n",
    "\n",
    "    # Preprocess input\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n",
    "    \n",
    "    # Apply model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Class probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n",
    "\n",
    "    # Sort classes by probabilities in descending order\n",
    "    sorted_indices = torch.argsort(probs, descending=True)\n",
    "    \n",
    "    return sorted_indices.tolist()  # Returns the class indices in descending order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863433b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate predictions\n",
    "true_labels = []\n",
    "top1_correct = 0\n",
    "top2_correct = 0\n",
    "top3_correct = 0\n",
    "top4_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Iterate with a progress bar\n",
    "idx=0\n",
    "for example in tqdm(validation_dataset, desc=\"Processing validation set\"):\n",
    "    idx=idx+1\n",
    "    #if idx==20:\n",
    "    #    break\n",
    "    audio_filepath = example[\"file_path\"]\n",
    "    true_label = label2id[example[\"label\"]]\n",
    "    #print(true_label)\n",
    "    \n",
    "    # Prediction from the model\n",
    "    sorted_predictions = predict_top(audio_filepath, model)\n",
    "    \n",
    "    # Check if the correct label is in the predictions\n",
    "    total_samples += 1\n",
    "    true_labels.append(true_label)\n",
    "    if true_label == sorted_predictions[0]:  # Top-1 correct\n",
    "        top1_correct += 1\n",
    "    if true_label in sorted_predictions[:2]:  # Top-2 correct\n",
    "        top2_correct += 1\n",
    "    if true_label in sorted_predictions[:3]:  # Top-3 correct\n",
    "        top3_correct += 1\n",
    "    if true_label in sorted_predictions[:4]:  # Top-4 correct\n",
    "        top4_correct += 1\n",
    "\n",
    "# Calculate Top-k Accuracy\n",
    "top1_accuracy = top1_correct / total_samples\n",
    "top2_accuracy = top2_correct / total_samples\n",
    "top3_accuracy = top3_correct / total_samples\n",
    "top4_accuracy = top4_correct / total_samples\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}\")\n",
    "print(f\"Top-2 Accuracy: {top2_accuracy:.2f}\")\n",
    "print(f\"Top-3 Accuracy: {top3_accuracy:.2f}\")\n",
    "print(f\"Top-4 Accuracy: {top4_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929491fa",
   "metadata": {},
   "source": [
    "# per-file evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize the data list\n",
    "data = []\n",
    "\n",
    "# Iterate through the dataset\n",
    "for example in validation_dataset:\n",
    "    audio_filepath = example[\"file_path\"]\n",
    "    true_label = label2id[example[\"label\"]]\n",
    "    sorted_predictions = predict_top(audio_filepath, model)\n",
    "    \n",
    "    # Extract filename\n",
    "    basename = os.path.basename(audio_filepath)\n",
    "    \n",
    "    # Extract the part up to the second \"_\" for test_id\n",
    "    parts = basename.split(\"_\")\n",
    "    if len(parts) > 2:\n",
    "        test_id = parts[0]+\"_\"+parts[1]  # Part up to the second \"_\"\n",
    "    else:\n",
    "        test_id = \"N/A\"  # If not found, mark it\n",
    "\n",
    "    # Add data to the list\n",
    "    data.append({\n",
    "        \"file_path\": audio_filepath,\n",
    "        \"test_id\": test_id,\n",
    "        \"true_label\": true_label,\n",
    "        \"top1_prediction\": sorted_predictions[0],\n",
    "        \"top2_prediction\": sorted_predictions[1] if len(sorted_predictions) > 1 else None,\n",
    "        \"top3_prediction\": sorted_predictions[2] if len(sorted_predictions) > 2 else None,\n",
    "        \"top4_prediction\": sorted_predictions[3] if len(sorted_predictions) > 3 else None,\n",
    "    })\n",
    "\n",
    "# Convert data to DataFrame format\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "df_results.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d848d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by test_id to aggregate results\n",
    "grouped_results = df_results.groupby(\"test_id\").agg(\n",
    "    true_label=(\"true_label\", lambda x: x.iloc[0]),  # Take the first true label\n",
    "    predicted_label=(\"top1_prediction\", lambda x: x.mode()[0] if not x.mode().empty else \"N/A\")  # Take the most frequent top-1 prediction (mode)\n",
    ").reset_index()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (grouped_results[\"true_label\"] == grouped_results[\"predicted_label\"]).mean()\n",
    "precision = grouped_results.groupby(\"predicted_label\").apply(\n",
    "    lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).sum() / len(x)\n",
    ").mean()\n",
    "recall = grouped_results.groupby(\"true_label\").apply(\n",
    "    lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).sum() / len(x)\n",
    ").mean()\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\n",
    "# Print metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1\n",
    "}\n",
    "\n",
    "metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f08870",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3716fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Numeric class labels\n",
    "true_labels = grouped_results[\"true_label\"].values\n",
    "predicted_labels = grouped_results[\"predicted_label\"].values\n",
    "\n",
    "# Calculate metrics for individual classes\n",
    "class_labels = list(id2label.keys())  # Class indices\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "# Binarize true labels for ROC-AUC calculation (one vs. all)\n",
    "true_labels_binarized = np.eye(len(class_labels))[true_labels]  # One-hot encoding\n",
    "predicted_labels_binarized = np.eye(len(class_labels))[predicted_labels]\n",
    "\n",
    "roc_auc_per_class = []\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels_binarized[:, i], predicted_labels_binarized[:, i])\n",
    "    except ValueError:  # If there is no positive sample\n",
    "        roc_auc = np.nan\n",
    "    roc_auc_per_class.append(roc_auc)\n",
    "\n",
    "# Print class-wise metrics\n",
    "print(\"\\nClass-wise metrics:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {id2label[label]}:\")\n",
    "    print(f\"  Precision: {precision_per_class[i]:.2f}\")\n",
    "    print(f\"  Recall: {recall_per_class[i]:.2f}\")\n",
    "    print(f\"  F1 Score: {f1_per_class[i]:.2f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_per_class[i]:.2f}\" if not np.isnan(roc_auc_per_class[i]) else \"  ROC-AUC: N/A\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=id2label.values(), yticklabels=id2label.values())\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization (class-wise metrics)\n",
    "metrics_df = {\n",
    "    \"Precision\": precision_per_class,\n",
    "    \"Recall\": recall_per_class,\n",
    "    \"F1 Score\": f1_per_class,\n",
    "    \"ROC-AUC\": roc_auc_per_class,\n",
    "}\n",
    "metrics_df = {id2label[label]: values for label, values in zip(class_labels, zip(*metrics_df.values()))}\n",
    "\n",
    "# Correct visualization of class-wise metrics\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_labels = [\"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "metrics_values = [precision_per_class, recall_per_class, f1_per_class, roc_auc_per_class]\n",
    "\n",
    "for metric, values in zip(metrics_labels, metrics_values):\n",
    "    plt.plot(class_labels, values, marker='o', label=metric)\n",
    "\n",
    "plt.xticks(class_labels, [id2label[label] for label in class_labels], rotation=45)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Class-wise Metrics\")\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901dc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
