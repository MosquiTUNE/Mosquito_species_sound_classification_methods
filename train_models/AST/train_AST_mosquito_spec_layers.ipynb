{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3eb85-3232-4181-bc61-180825011659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall datasets -y\n",
    "#!pip install datasets\n",
    "# test huffingface, transformers is installed\n",
    "#from transformers import pipeline\n",
    "#classifier = pipeline(\"sentiment-analysis\")\n",
    "#print(classifier(\"This is amazing!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aaa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num=40\n",
    "batch_size=4\n",
    "train_dir=\"./train-val-independent/train/\"\n",
    "validation_dir=\"./train-val-independent/validation/\"\n",
    "everyNth=1 # we train on every Nth data point, because all of it doesn't fit into memory.\n",
    "postfix=\"-AST_multiclass_mosquito_indep_\"\n",
    "b_train_just_last=False\n",
    "trainNlayers=3 # train last N layers. if 0, then train full network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b06a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40776082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_dataset_from_folders(train_dir, validation_dir):\n",
    "    \"\"\"\n",
    "    Load data from folders and convert to Dataset format.\n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Path to the 'train' folder.\n",
    "        validation_dir (str): Path to the 'validation' folder.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A DatasetDict with 'train' and 'validation' data.\n",
    "    \"\"\"\n",
    "    def get_audio_files_with_labels(directory):\n",
    "        data = []\n",
    "        for class_name in os.listdir(directory):  # Classes ('mosquito', 'not')\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\".wav\"):  # Only WAV files\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        data.append({\"file_path\": file_path, \"label\": class_name})\n",
    "        return data\n",
    "\n",
    "    # Load train and validation data\n",
    "    train_data = get_audio_files_with_labels(train_dir)\n",
    "    validation_data = get_audio_files_with_labels(validation_dir)\n",
    "\n",
    "    # Create Dataset\n",
    "    \n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"file_path\": [d[\"file_path\"] for idx, d in enumerate(train_data) if idx % everyNth == 0],\n",
    "        \"label\": [d[\"label\"] for idx, d in enumerate(train_data) if idx % everyNth == 0]\n",
    "    })\n",
    "\n",
    "    validation_dataset = Dataset.from_dict({\n",
    "        \"file_path\": [d[\"file_path\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0],\n",
    "        \"label\": [d[\"label\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0]\n",
    "    })\n",
    "\n",
    "    return DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_audio_dataset_from_folders(train_dir, validation_dir)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=set()\n",
    "\n",
    "# Iterate through the subdirectories\n",
    "for species in os.listdir(train_dir):\n",
    "    label_list.add(species)\n",
    "label2id = {key: idx for idx, key in enumerate(label_list)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "cls_num=len(id2label)\n",
    "print(id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"]\n",
    "print(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f88ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Class labels: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
    "processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "    num_labels=cls_num,  # Number of classes\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "ignore_mismatched_sizes=True)\n",
    "\n",
    "if b_train_just_last:\n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Only leave the classification layer trainable\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "elif trainNlayers>0:\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Make the last N blocks trainable (e.g., the last 2)\n",
    "    for layer in model.audio_spectrogram_transformer.encoder.layer[-trainNlayers:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "else:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8297af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model and inputs to the GPU\n",
    "model = model.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faaa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_function(examples, target_length=16000):\n",
    "    \"\"\"\n",
    "    Audio preprocessing function adapted for batch processing.\n",
    "    \"\"\"\n",
    "    input_values = []\n",
    "    labels = []\n",
    "\n",
    "    for audio_fn, label in zip(examples['file_path'], examples['label']):\n",
    "        # Use librosa to load the file and resample\n",
    "        audio, sr = librosa.load(audio_fn, sr=16000)  # Resample to 16 kHz\n",
    "\n",
    "        # Pad short audio files\n",
    "        if len(audio) < target_length:\n",
    "            padding = target_length - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode=\"constant\")\n",
    "        elif len(audio) > target_length:\n",
    "            audio = audio[:target_length]\n",
    "\n",
    "        # Apply processor\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Store\n",
    "        input_values.append(inputs[\"input_values\"].squeeze().numpy())\n",
    "        labels.append(int(label2id[label]))\n",
    "\n",
    "    return {\"input_values\": input_values, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a992f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_dataset = train_dataset.map(preprocess_audio_function,batched=True,batch_size=4,)\n",
    "encoded_validation_dataset = validation_dataset.map(preprocess_audio_function,batched=True,batch_size=4,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    balanced_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"balanced_accuracy\": balanced_accuracy\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ae7de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\" + postfix,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=5e-5,  # Increased initial learning rate (can be reduced to 2e-5 if unstable)\n",
    "    lr_scheduler_type=\"cosine\",  # Learning rate scheduler (gradual decrease during training)\n",
    "    warmup_steps=3,  # Number of warmup steps for stability\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,  # Evaluation batch size\n",
    "    num_train_epochs=epoch_num,\n",
    "    save_strategy=\"epoch\",  # Save at the end of each epoch\n",
    "    save_total_limit=2,  # Only keep 2 model saves\n",
    "    logging_dir='./logs' + postfix,\n",
    "    logging_steps=10,  # Log less frequently per step\n",
    "    report_to=\"all\",  # Log to console and file\n",
    "    #load_best_model_at_end=True,  # Automatically load the best model at the end of training\n",
    "    metric_for_best_model=\"balanced_accuracy\",  # Metric used to select the best model\n",
    "    greater_is_better=True,  # Higher metric values are better\n",
    "    gradient_accumulation_steps=8,  # Accumulated gradient count (larger effective batch size)\n",
    "    fp16=True,  # Mixed precision training for faster training (if hardware supports it)\n",
    "    save_steps=10,  # Save steps (if evaluation_strategy is not \"epoch\")\n",
    "    dataloader_num_workers=4,  # Faster loading using multiple threads\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_validation_dataset,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,  # Compute metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./classifier\"+postfix)\n",
    "processor.save_pretrained(\"./classifier-\"+postfix)\n",
    "\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53241ad2-adef-4c27-8712-374cf16f6905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf611870-44b0-4f57-97a4-31e44ab29cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
