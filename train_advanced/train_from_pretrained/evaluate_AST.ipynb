{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_dir = \"../../dataset/train-val_mosquito_sounds_classification_3cls_25_02_14/validation/\"\n",
    "\n",
    "checkpoint_dir = \"./ensemble/best/_independentdata_seed42_adamw_cosine/\"  # Best checkpoint directory\n",
    "\n",
    "id2label = {\n",
    "    0: 'Aedes_albopictus',\n",
    "    1: 'Aedes_koreicus',\n",
    "    2: 'Ochlerotatus_geniculatus'\n",
    "}\n",
    "\n",
    "cls_num = len(id2label)\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from safetensors.torch import load_file\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "print(\"Does the checkpoint folder exist?\", os.path.exists(checkpoint_dir))\n",
    "print(\"Folder contents:\", os.listdir(checkpoint_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_dataset_from_folders(validation_dir, everyNth=1):\n",
    "    \"\"\"\n",
    "    Load audio data from folders and convert to a Hugging Face Dataset.\n",
    "\n",
    "    Args:\n",
    "        validation_dir (str): Path to the 'validation' folder.\n",
    "        everyNth (int): Optional downsampling (take every nth file).\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Dataset object with file paths and labels.\n",
    "    \"\"\"\n",
    "    def get_audio_files_with_labels(directory):\n",
    "        data = []\n",
    "        for class_name in os.listdir(directory):  # Class folders\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\".wav\"):  # Only WAV files\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        data.append({\"file_path\": file_path, \"label\": class_name})\n",
    "        return data\n",
    "\n",
    "    validation_data = get_audio_files_with_labels(validation_dir)\n",
    "\n",
    "    validation_dataset = Dataset.from_dict({\n",
    "        \"file_path\": [d[\"file_path\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0],\n",
    "        \"label\": [d[\"label\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0]\n",
    "    })\n",
    "    return validation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "validation_dataset = load_audio_dataset_from_folders(validation_dir)\n",
    "\n",
    "print(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a39852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base AST model\n",
    "processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "base_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# Adaptation for SSL (just like before)\n",
    "class AST_SSL(nn.Module):\n",
    "    def __init__(self, base_model, output_dim):\n",
    "        super(AST_SSL, self).__init__()\n",
    "        self.encoder = base_model\n",
    "        self.encoder_output_dim = base_model.config.hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.projector = nn.Linear(self.encoder_output_dim, output_dim[-1])\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(output_dim[-1], output_dim[-1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim[-1], output_dim[-1], kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.encoder(input_values=input_values).last_hidden_state\n",
    "        projected = self.projector(outputs)\n",
    "\n",
    "        # Reshape for temporal decoder\n",
    "        projected = projected.permute(0, 2, 1)  # (B, T, C) -> (B, C, T)\n",
    "        reconstructed = self.decoder(projected).permute(0, 2, 1)  # Back to (B, T, C)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Interpolate reconstructed output to match labels shape\n",
    "            reconstructed = F.interpolate(\n",
    "                reconstructed.permute(0, 2, 1),\n",
    "                size=labels.shape[1], mode=\"linear\", align_corners=True\n",
    "            ).permute(0, 2, 1)\n",
    "\n",
    "            loss_fn = nn.MSELoss()\n",
    "            loss = loss_fn(reconstructed, labels)\n",
    "            return loss, reconstructed\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "# Instantiate SSL base model\n",
    "ssl_model = AST_SSL(base_model, output_dim=[768])\n",
    "\n",
    "# Remove projector/decoder layers, keeping only the encoder\n",
    "del ssl_model.projector\n",
    "del ssl_model.decoder\n",
    "\n",
    "# Add new classification layer\n",
    "class AST_Classifier(nn.Module):\n",
    "    def __init__(self, ssl_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = ssl_model.encoder  # Keep AST encoder part\n",
    "        self.classifier = nn.Linear(768, num_classes)  # New classification layer\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.encoder(input_values)\n",
    "        x = outputs.last_hidden_state\n",
    "        x = self.encoder.layernorm(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling over time (B, D)\n",
    "        logits = self.classifier(x)  # Classification logits (B, num_classes)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "    \n",
    "classifier_model = AST_Classifier(ssl_model, num_classes=cls_num)\n",
    "print(\"New classifier model created:\", classifier_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = load_file(checkpoint_dir + \"model.safetensors\")\n",
    "classifier_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"âœ… Model loaded from checkpoint!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier_model\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58178db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(audio_filepath, model, processor, device):\n",
    "    \"\"\"\n",
    "    Load an audio file, preprocess it, and predict its class using the loaded model.\n",
    "\n",
    "    :param audio_filepath: Path to the audio file\n",
    "    :param model: Loaded AST-based classifier model\n",
    "    :param processor: Transformer processor for preprocessing\n",
    "    :param device: 'cuda' or 'cpu'\n",
    "    :return: (predicted_class_index, predicted_probability)\n",
    "    \"\"\"\n",
    "    audio_waveform, _ = librosa.load(audio_filepath, sr=16000)\n",
    "\n",
    "    inputs = processor(audio_waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "\n",
    "    logits = outputs[\"logits\"]\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "    return predicted_class, probabilities[0][predicted_class].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a52063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for example in tqdm(validation_dataset, desc=\"Processing validation set\"):\n",
    "    audio_filepath = example[\"file_path\"]\n",
    "    true_label = example[\"label\"]\n",
    "    predicted_class, predicted_prob = predict(audio_filepath, model, processor, device)\n",
    "    true_labels.append(true_label)\n",
    "    predicted_labels.append(predicted_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffad199",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [label2id[x] for x in true_labels]\n",
    "\n",
    "true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1beb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy*100:.4f}\")\n",
    "print(f\"Precision: {precision*100:.4f}\")\n",
    "print(f\"Recall: {recall*100:.4f}\")\n",
    "print(f\"F1 Score: {f1*100:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=id2label.values(), yticklabels=id2label.values())\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c579e",
   "metadata": {},
   "source": [
    "# Class-wise metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b527dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-wise metrics\n",
    "class_labels = list(id2label.keys())\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "true_labels_binarized = np.eye(len(class_labels))[true_labels]\n",
    "predicted_labels_binarized = np.eye(len(class_labels))[predicted_labels]\n",
    "\n",
    "roc_auc_per_class = []\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels_binarized[:, i], predicted_labels_binarized[:, i])\n",
    "    except ValueError:\n",
    "        roc_auc = np.nan\n",
    "    roc_auc_per_class.append(roc_auc)\n",
    "\n",
    "print(\"\\nClass-wise metrics:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {id2label[label]}:\")\n",
    "    print(f\"  Precision: {precision_per_class[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_per_class[i]:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_per_class[i]:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_per_class[i]:.4f}\" if not np.isnan(roc_auc_per_class[i]) else \"  ROC-AUC: N/A\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_labels = [\"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "metrics_values = [precision_per_class, recall_per_class, f1_per_class, roc_auc_per_class]\n",
    "\n",
    "for metric, values in zip(metrics_labels, metrics_values):\n",
    "    plt.plot(class_labels, values, marker='o', label=metric)\n",
    "\n",
    "plt.xticks(class_labels, [id2label[label] for label in class_labels], rotation=45)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Class-wise Metrics\")\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9545c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929491fa",
   "metadata": {},
   "source": [
    "# per file metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def predict_top(audio_filepath, model, processor, device):\n",
    "    \"\"\"\n",
    "    Load an audio file, preprocess it, and return all class indices in descending probability order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_filepath : str\n",
    "        Path to the audio file (.wav).\n",
    "    model : nn.Module\n",
    "        Loaded AST-based classifier model.\n",
    "    processor : AutoProcessor\n",
    "        Hugging Face processor for preprocessing the waveform.\n",
    "    device : str or torch.device\n",
    "        Device to run inference on (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_predictions : list of int\n",
    "        Class indices sorted by descending probability.\n",
    "    sorted_probs : list of float\n",
    "        Corresponding probabilities for each class index.\n",
    "    \"\"\"\n",
    "    model.to(device)  # Make sure the model is on the correct device\n",
    "\n",
    "    # Load and resample audio to 16 kHz\n",
    "    audio_waveform, _ = librosa.load(audio_filepath, sr=16000)\n",
    "\n",
    "    # Preprocess with transformer processor\n",
    "    inputs = processor(audio_waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()  # Disable dropout for inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "\n",
    "    logits = outputs[\"logits\"]  # Raw scores\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy().flatten()  # Convert to probabilities\n",
    "\n",
    "    # Sort class indices by descending probability\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]\n",
    "    sorted_predictions = [int(idx) for idx in sorted_indices]\n",
    "    sorted_probs = [float(probabilities[idx]) for idx in sorted_indices]\n",
    "\n",
    "    return sorted_predictions, sorted_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Collect per-file results\n",
    "data = []\n",
    "\n",
    "for example in validation_dataset:\n",
    "    audio_filepath = example[\"file_path\"]\n",
    "    true_label = label2id[example[\"label\"]]\n",
    "\n",
    "    # Predict top classes for each file\n",
    "    sorted_predictions, sorted_probs = predict_top(audio_filepath, model, processor, device)\n",
    "\n",
    "    # Extract a test_id from the file name up to the second underscore\n",
    "    basename = os.path.basename(audio_filepath)\n",
    "    parts = basename.split(\"_\")\n",
    "    test_id = parts[0] + \"_\" + parts[1] if len(parts) > 2 else \"N/A\"\n",
    "\n",
    "    data.append({\n",
    "        \"file_path\": audio_filepath,\n",
    "        \"test_id\": test_id,\n",
    "        \"true_label\": true_label,\n",
    "        \"top1_prediction\": sorted_predictions[0],\n",
    "        \"top2_prediction\": sorted_predictions[1] if len(sorted_predictions) > 1 else None,\n",
    "        \"top3_prediction\": sorted_predictions[2] if len(sorted_predictions) > 2 else None,\n",
    "        \"top4_prediction\": sorted_predictions[3] if len(sorted_predictions) > 3 else None,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(data)\n",
    "df_results.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d848d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results per test_id to get majority-vote predictions\n",
    "grouped_results = df_results.groupby(\"test_id\").agg(\n",
    "    true_label=(\"true_label\", lambda x: x.iloc[0]),  # Take the first true label for the group\n",
    "    predicted_label=(\"top1_prediction\", lambda x: x.mode()[0] if not x.mode().empty else \"N/A\")  # Most frequent top1 prediction\n",
    ").reset_index()\n",
    "\n",
    "# Compute overall metrics\n",
    "accuracy = (grouped_results[\"true_label\"] == grouped_results[\"predicted_label\"]).mean()\n",
    "precision = grouped_results.groupby(\"predicted_label\").apply(\n",
    "    lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).sum() / len(x)\n",
    ").mean()\n",
    "recall = grouped_results.groupby(\"true_label\").apply(\n",
    "    lambda x: (x[\"true_label\"] == x[\"predicted_label\"]).sum() / len(x)\n",
    ").mean()\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1\n",
    "}\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f08870",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3716fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "true_labels = grouped_results[\"true_label\"].values\n",
    "predicted_labels = grouped_results[\"predicted_label\"].values\n",
    "\n",
    "class_labels = list(id2label.keys())\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "true_labels_binarized = np.eye(len(class_labels))[true_labels]\n",
    "predicted_labels_binarized = np.eye(len(class_labels))[predicted_labels]\n",
    "\n",
    "roc_auc_per_class = []\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels_binarized[:, i], predicted_labels_binarized[:, i])\n",
    "    except ValueError:\n",
    "        roc_auc = np.nan\n",
    "    roc_auc_per_class.append(roc_auc)\n",
    "\n",
    "print(\"\\nClass-wise metrics:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {id2label[label]}:\")\n",
    "    print(f\"  Precision: {precision_per_class[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_per_class[i]:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_per_class[i]:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_per_class[i]:.4f}\" if not np.isnan(roc_auc_per_class[i]) else \"  ROC-AUC: N/A\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=id2label.values(), yticklabels=id2label.values())\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot per-class metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_labels = [\"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "metrics_values = [precision_per_class, recall_per_class, f1_per_class, roc_auc_per_class]\n",
    "\n",
    "for metric, values in zip(metrics_labels, metrics_values):\n",
    "    plt.plot(class_labels, values, marker='o', label=metric)\n",
    "\n",
    "plt.xticks(class_labels, [id2label[label] for label in class_labels], rotation=45)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Class-wise Metrics\")\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901dc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
