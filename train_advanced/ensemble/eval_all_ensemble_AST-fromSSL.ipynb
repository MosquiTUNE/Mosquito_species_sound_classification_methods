{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir = \"../dataset/train-val-random/validation/\"\n",
    "\n",
    "checkpoints_dir = \"./ensemble/best/\" # inside, there are many checkpoints from which an ensemble should be calculated\n",
    "\n",
    "cls_num = 3  # Set the appropriate value here\n",
    "#id2label={0: 'Aedes_koreicus', 1: 'Ochlerotatus_geniculatus', 2: 'Aedes_albopictus'}\n",
    "id2label={0: 'Aedes_koreicus', 1: 'Aedes_albopictus', 2: 'Ochlerotatus_geniculatus'}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from safetensors.torch import load_file\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ce2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists all subdirectories in the specified directory\n",
    "checkpoints = [os.path.join(checkpoints_dir, d) for d in os.listdir(checkpoints_dir) \n",
    "               if os.path.isdir(os.path.join(checkpoints_dir, d))]\n",
    "\n",
    "print(\"Found checkpoints:\", checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_dataset_from_folders(validation_dir, everyNth=1):\n",
    "    \"\"\"\n",
    "    Load data from folders and convert to Dataset format.\n",
    "\n",
    "    Args:\n",
    "        validation_dir (str): Path to the 'validation' folder.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: A DatasetDict with 'validation' data.\n",
    "    \"\"\"\n",
    "    def get_audio_files_with_labels(directory):\n",
    "        data = []\n",
    "        for class_name in os.listdir(directory):  # Classes ('mosquito', 'not')\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\".wav\"):  # Only WAV files\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        data.append({\"file_path\": file_path, \"label\": class_name})\n",
    "        return data\n",
    "\n",
    "    # Load validation data\n",
    "    validation_data = get_audio_files_with_labels(validation_dir)\n",
    "\n",
    "    # Create Dataset\n",
    "\n",
    "    validation_dataset = Dataset.from_dict({\n",
    "        \"file_path\": [d[\"file_path\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0],\n",
    "        \"label\": [d[\"label\"] for idx, d in enumerate(validation_data) if idx % everyNth == 0]\n",
    "    })\n",
    "\n",
    "    #return DatasetDict({\"validation\": validation_dataset})\n",
    "    return validation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "validation_dataset = load_audio_dataset_from_folders(validation_dir)\n",
    "\n",
    "print(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a39852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Load the base AST model\n",
    "processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "base_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "\"\"\"\n",
    "# Convert for SSL\n",
    "class AST_SSL(nn.Module):\n",
    "    def __init__(self, base_model, output_dim):\n",
    "        super(AST_SSL, self).__init__()\n",
    "        self.encoder = base_model\n",
    "        self.encoder_output_dim = base_model.config.hidden_size\n",
    "\n",
    "        # Input and output dimension check\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Projector - Linear transformation on the AST hidden representation\n",
    "        self.projector = nn.Linear(self.encoder_output_dim, output_dim[-1])\n",
    "\n",
    "        # Convolutional decoder for temporal reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(output_dim[-1], output_dim[-1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim[-1], output_dim[-1], kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.encoder(input_values=input_values).last_hidden_state\n",
    "        projected = self.projector(outputs)\n",
    "\n",
    "        # Reshape for temporal decoder\n",
    "        projected = projected.permute(0, 2, 1)  # (B, T, C) -> (B, C, T)\n",
    "        reconstructed = self.decoder(projected).permute(0, 2, 1)  # Back to (B, T, C)\n",
    "\n",
    "        if labels is not None:\n",
    "            # **Interpolation to the shape of labels**\n",
    "            reconstructed = F.interpolate(reconstructed.permute(0, 2, 1), \n",
    "                                          size=labels.shape[1], mode=\"linear\", align_corners=True)\n",
    "            reconstructed = reconstructed.permute(0, 2, 1)\n",
    "            \n",
    "            loss_fn = nn.MSELoss()\n",
    "            loss = loss_fn(reconstructed, labels)\n",
    "            return loss, reconstructed\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "class AST_Classifier(nn.Module):\n",
    "    def __init__(self, ssl_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = ssl_model.encoder  # We keep the AST encoder part\n",
    "        #self.layernorm = ssl_model.encoder.layernorm  # LayerNorm remains\n",
    "        self.classifier = nn.Linear(768, num_classes)  # New classification layer\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        # The encoder's output is a ModelOutput, from which we need to select last_hidden_state.\n",
    "        outputs = self.encoder(input_values)\n",
    "        x = outputs.last_hidden_state  # This is already a Tensor\n",
    "        x = self.encoder.layernorm(x)\n",
    "        x = x.mean(dim=1)  # Global pooling: temporal averaging (B, D)\n",
    "        logits = self.classifier(x)  # Classification logits, shape: (B, num_classes)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # If labels are also provided, calculate cross-entropy loss\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "        return {\"logits\": logits}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58178db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def predict(audio_filepath, model, processor, device):\n",
    "    \"\"\"\n",
    "    Loads the audio file, preprocesses it, and predicts the category with the loaded model.\n",
    "    \n",
    "    :param audio_filepath: The path to the audio file\n",
    "    :param model: The loaded AST-based classifier model\n",
    "    :param processor: The Transformer processor for preprocessing\n",
    "    :return: (predicted_class, probability) - The index of the predicted class and its probability\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and normalize the audio file\n",
    "    audio_waveform, sample_rate = librosa.load(audio_filepath, sr=16000)  # sampled at 16kHz\n",
    "\n",
    "    # Transformer preprocessing\n",
    "    inputs = processor(audio_waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values\n",
    "    input_values = inputs.input_values.to(device)  # Send to the same device as the model\n",
    "\n",
    "\n",
    "    # Model prediction\n",
    "    model.eval()  # Turn off dropout layers (if any)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    \n",
    "    logits = outputs[\"logits\"]  # Extract the logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)  # Softmax normalization\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()  # Index of the most probable class\n",
    "\n",
    "    return predicted_class, probabilities[0][predicted_class].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_checkpoints(checkpoints_dir, validation_dataset, label2id, cls_num, predict):\n",
    "    \"\"\" \n",
    "    Loads all checkpoints from the specified directory, \n",
    "    evaluates them on the validation data, and collects the results.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all available checkpoint directories\n",
    "    checkpoints = [os.path.join(checkpoints_dir, d) for d in os.listdir(checkpoints_dir) \n",
    "                   if os.path.isdir(os.path.join(checkpoints_dir, d))]\n",
    "\n",
    "    print(f\"Found {len(checkpoints)} checkpoints!\")\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the base AST model\n",
    "    processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "    base_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "    # Initialize the DataFrame to store results\n",
    "    all_predictions = pd.DataFrame(columns=[\"file_path\", \"true_label\"] + [f\"pred_{i}\" for i in range(len(checkpoints))])\n",
    "\n",
    "    # Iterate through all checkpoints\n",
    "    for idx, checkpoint_dir in enumerate(checkpoints):\n",
    "        print(f\"ðŸ”„ Evaluating with checkpoint {checkpoint_dir}...\")\n",
    "\n",
    "        # Create SSL base model\n",
    "        ssl_model = AST_SSL(base_model, output_dim=[768])\n",
    "        del ssl_model.projector\n",
    "        del ssl_model.decoder\n",
    "        # Initialize classification model\n",
    "        classifier_model = AST_Classifier(ssl_model, num_classes=cls_num)\n",
    "        classifier_model.to(device)\n",
    "\n",
    "        # Load model from checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"model.safetensors\")\n",
    "        state_dict = load_file(checkpoint_path)\n",
    "        classifier_model.load_state_dict(state_dict, strict=False)\n",
    "        classifier_model.eval()  # Switch to evaluation mode\n",
    "        print(\"âœ… Model loaded!\")\n",
    "\n",
    "        # Calculate predictions\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        for example in tqdm(validation_dataset, desc=f\"Checkpoint {idx+1}/{len(checkpoints)}\"):\n",
    "            audio_filepath = example[\"file_path\"]\n",
    "            true_label = example[\"label\"]\n",
    "            predicted_class, _ = predict(audio_filepath, classifier_model, processor, device)\n",
    "            \n",
    "            true_labels.append(label2id[true_label])\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        # If this is the first checkpoint, create a base DataFrame\n",
    "        if idx == 0:\n",
    "            all_predictions[\"file_path\"] = [example[\"file_path\"] for example in validation_dataset]\n",
    "            all_predictions[\"true_label\"] = true_labels\n",
    "\n",
    "        # Add a new column with the predictions of the current checkpoint\n",
    "        all_predictions[f\"pred_{idx}\"] = predictions\n",
    "        \n",
    "        all_predictions.to_csv(\"ensemble_results.csv\")\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = \"./ensemble/best/\"\n",
    "results_df = evaluate_checkpoints(checkpoints_dir, validation_dataset, label2id, cls_num, predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7909f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, the labels got mixed up due to the use of \"set\".\n",
    "# they need to be corrected afterwards\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def remap_predictions_simple(df):\n",
    "    true_labels = df[\"true_label\"].values\n",
    "    pred_cols = [col for col in df.columns if col.startswith(\"pred_\")]\n",
    "    \n",
    "    for col in pred_cols:\n",
    "        pred_labels = df[col].values\n",
    "        \n",
    "        # We count which label is the most frequent pair between true_label and pred_label\n",
    "        mapping = {}\n",
    "        for true, pred in zip(true_labels, pred_labels):\n",
    "            if pred not in mapping:\n",
    "                mapping[pred] = Counter()\n",
    "            mapping[pred][true] += 1\n",
    "        \n",
    "        # For each pred value, we assign the true_label value that is the most frequent\n",
    "        best_mapping = {pred: max(counts, key=counts.get) for pred, counts in mapping.items()}\n",
    "        \n",
    "        # Apply the mapping to the column\n",
    "        df[col] = df[col].map(best_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run\n",
    "updated_df = remap_predictions_simple(results_df)\n",
    "\n",
    "# Save CSV\n",
    "updated_df.to_csv(\"ensemble_results.csv\", index=False)\n",
    "\n",
    "updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9392ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# Calculate the ensemble prediction (majority vote)\n",
    "pred_cols = [col for col in updated_df.columns if col.startswith(\"pred_\")]\n",
    "ensemble_predictions = mode(updated_df[pred_cols].values, axis=1)[0].flatten()\n",
    "\n",
    "# True labels\n",
    "true_labels = updated_df[\"true_label\"].values\n",
    "\n",
    "# Calculate metrics for individual models and the ensemble\n",
    "metrics = {\"Model\": [], \"Accuracy\": [], \"Balanced Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1 Score\": []}\n",
    "\n",
    "# Evaluate individual models\n",
    "for col in pred_cols:\n",
    "    predicted_labels = updated_df[col].values\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "    \n",
    "    metrics[\"Model\"].append(col)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"Balanced Accuracy\"].append(balanced_accuracy)\n",
    "    metrics[\"Precision\"].append(precision)\n",
    "    metrics[\"Recall\"].append(recall)\n",
    "    metrics[\"F1 Score\"].append(f1)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "accuracy = accuracy_score(true_labels, ensemble_predictions)\n",
    "balanced_accuracy = balanced_accuracy_score(true_labels, ensemble_predictions)\n",
    "precision = precision_score(true_labels, ensemble_predictions, average=\"weighted\", zero_division=0)\n",
    "recall = recall_score(true_labels, ensemble_predictions, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(true_labels, ensemble_predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "metrics[\"Model\"].append(\"Ensemble\")\n",
    "metrics[\"Accuracy\"].append(accuracy)\n",
    "metrics[\"Balanced Accuracy\"].append(balanced_accuracy)\n",
    "metrics[\"Precision\"].append(precision)\n",
    "metrics[\"Recall\"].append(recall)\n",
    "metrics[\"F1 Score\"].append(f1)\n",
    "\n",
    "# Print metrics\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n",
    "\n",
    "# Calculate confusion matrix for the ensemble model\n",
    "conf_matrix = confusion_matrix(true_labels, ensemble_predictions)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Ensemble Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceafa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERRORROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffad199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1beb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=id2label.values(), yticklabels=id2label.values())\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b527dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for individual classes\n",
    "class_labels = list(id2label.keys())  # Class indices\n",
    "precision_per_class = precision_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(true_labels, predicted_labels, average=None, zero_division=0)\n",
    "\n",
    "# Binarize true labels for ROC-AUC calculation (one vs. all)\n",
    "true_labels_binarized = np.eye(len(class_labels))[true_labels]  # One-hot encoding\n",
    "predicted_labels_binarized = np.eye(len(class_labels))[predicted_labels]\n",
    "\n",
    "roc_auc_per_class = []\n",
    "for i in range(len(class_labels)):\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(true_labels_binarized[:, i], predicted_labels_binarized[:, i])\n",
    "    except ValueError:  # If there is no positive sample\n",
    "        roc_auc = np.nan\n",
    "    roc_auc_per_class.append(roc_auc)\n",
    "\n",
    "# Print class-wise metrics\n",
    "print(\"\\nClass-wise metrics:\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {id2label[label]}:\")\n",
    "    print(f\"  Precision: {precision_per_class[i]:.2f}\")\n",
    "    print(f\"  Recall: {recall_per_class[i]:.2f}\")\n",
    "    print(f\"  F1 Score: {f1_per_class[i]:.2f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc_per_class[i]:.2f}\" if not np.isnan(roc_auc_per_class[i]) else \"  ROC-AUC: N/A\")\n",
    "\n",
    "# Visualization (class-wise metrics)\n",
    "metrics_df = {\n",
    "    \"Precision\": precision_per_class,\n",
    "    \"Recall\": recall_per_class,\n",
    "    \"F1 Score\": f1_per_class,\n",
    "    \"ROC-AUC\": roc_auc_per_class,\n",
    "}\n",
    "metrics_df = {id2label[label]: values for label, values in zip(class_labels, zip(*metrics_df.values()))}\n",
    "\n",
    "# Correct visualization of class-wise metrics\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_labels = [\"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "metrics_values = [precision_per_class, recall_per_class, f1_per_class, roc_auc_per_class]\n",
    "\n",
    "for metric, values in zip(metrics_labels, metrics_values):\n",
    "    plt.plot(class_labels, values, marker='o', label=metric)\n",
    "\n",
    "plt.xticks(class_labels, [id2label[label] for label in class_labels], rotation=45)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Class-wise Metrics\")\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9545c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
