{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic parameters\n",
    "data_dir = \"./dataset_forSSL_indep/\"  # Folder containing wav files: the entire ABUZZ audio plus the independent training part\n",
    "learning_rate = 1e-4  # Smaller learning rate for more stable SSL\n",
    "batch_size = 8  # Increased batch size (original was 4)\n",
    "epoch_num = 150\n",
    "str_id = \"_SSL_v3_indep\"\n",
    "resume_from_checkpoint = None  # None to start from scratch; or provide path to resume from a checkpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor, ASTModel\n",
    "import librosa\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers.trainer_utils import set_seed\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c7f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base AST model\n",
    "processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "base_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# Adaptation for SSL\n",
    "class AST_SSL(nn.Module):\n",
    "    def __init__(self, base_model, output_dim):\n",
    "        super(AST_SSL, self).__init__()\n",
    "        self.encoder = base_model\n",
    "        self.encoder_output_dim = base_model.config.hidden_size\n",
    "\n",
    "        # Store output dimensions\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Projector: linear transformation of AST hidden representation\n",
    "        self.projector = nn.Linear(self.encoder_output_dim, output_dim[-1])\n",
    "\n",
    "        # Convolutional decoder for temporal reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(output_dim[-1], output_dim[-1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim[-1], output_dim[-1], kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.encoder(input_values=input_values).last_hidden_state  # (B, T_enc, H)\n",
    "        projected = self.projector(outputs)                                   # (B, T_enc, C)\n",
    "        x = projected.permute(0, 2, 1)                                        # (B, C, T_enc)\n",
    "        reconstructed = self.decoder(x).permute(0, 2, 1)                      # (B, T_dec, C)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Align time axis: interpolate reconstructed sequence to match labels time dimension\n",
    "            rec = F.interpolate(\n",
    "                reconstructed.permute(0, 2, 1),\n",
    "                size=labels.shape[1],\n",
    "                mode=\"linear\",\n",
    "                align_corners=True\n",
    "            ).permute(0, 2, 1)\n",
    "            loss = F.mse_loss(rec, labels)\n",
    "            return {\"loss\": loss, \"logits\": rec}\n",
    "        return {\"logits\": reconstructed}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2860bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, processor, is_train=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.processor = processor\n",
    "        self.is_train = is_train\n",
    "        self.time_masking = T.TimeMasking(time_mask_param=80)\n",
    "        self.freq_masking = T.FrequencyMasking(freq_mask_param=30)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        audio, _ = librosa.load(file_path, sr=16000)\n",
    "\n",
    "        # Adjust audio length\n",
    "        target_length = 16000  # 1 second\n",
    "        if len(audio) < target_length:\n",
    "            padding = target_length - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode=\"constant\")\n",
    "        else:\n",
    "            audio = audio[:target_length]\n",
    "\n",
    "        # Processor preprocessing\n",
    "        inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        clean_input = inputs[\"input_values\"].squeeze()  # Save the original\n",
    "        inputs[\"output_values\"] = clean_input.clone()   # The target is to restore the clean spectrum\n",
    "\n",
    "        if self.is_train:\n",
    "            inputs[\"input_values\"] = self.augment_spectrogram(clean_input)\n",
    "        else:\n",
    "            inputs[\"input_values\"] = clean_input\n",
    "    \n",
    "        return inputs\n",
    "\n",
    "    def augment_spectrogram(self, input_values: torch.Tensor):\n",
    "        # input_values: (T, M) ~ (time, mel)\n",
    "        spec = input_values.transpose(0, 1).contiguous()   # (M, T) -> (freq, time)\n",
    "        spec = self.time_masking(spec)\n",
    "        spec = self.freq_masking(spec)\n",
    "        spec = spec.transpose(0, 1).contiguous()           # back to (T, M)\n",
    "        return spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f07695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90/10 data split\n",
    "all_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".wav\")]\n",
    "# all_files = all_files[:300]\n",
    "\n",
    "train_files, val_files = train_test_split(all_files, test_size=0.1, random_state=42)\n",
    "\n",
    "# Datasets\n",
    "train_dataset = AudioDataset(train_files, processor, is_train=True)\n",
    "val_dataset = AudioDataset(val_files, processor, is_train=False)\n",
    "\n",
    "# Check and store the dimensions of the first element\n",
    "sample_data = train_dataset[0]  # First element from the dataset\n",
    "input_shape = sample_data[\"input_values\"].shape\n",
    "output_shape = sample_data[\"output_values\"].shape\n",
    "\n",
    "print(f\"Input values shape: {input_shape}\")\n",
    "print(f\"Output values shape: {output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f041e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = input_shape  # Example input dimensions\n",
    "output_dim = output_shape  # Example target dimensions\n",
    "\n",
    "ssl_model = AST_SSL(base_model, output_dim=output_dim)\n",
    "\n",
    "if resume_from_checkpoint is not None:\n",
    "    ssl_model.load_state_dict(torch.load(os.path.join(resume_from_checkpoint, \"model.pth\")))\n",
    "    print(\"Model loaded from checkpoint.\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.asarray(preds)\n",
    "    labels = np.asarray(labels)\n",
    "    mse = ((preds - labels) ** 2).mean(dtype=np.float64)\n",
    "    return {\"eval_mse\": float(mse)}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_values = torch.stack([b[\"input_values\"] for b in batch])\n",
    "    labels = torch.stack([b[\"output_values\"] for b in batch])\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./AST-SSL-results\" + str_id,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epoch_num,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./AST-SSL-logs' + str_id,\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mse\",  # Must match the return key of compute_metrics\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # Mixed precision if GPU is available\n",
    "    gradient_accumulation_steps=2,  # If memory is low\n",
    "    dataloader_num_workers=4,  # Speed up data loading\n",
    "    warmup_ratio=0.05,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,     # Stop if no improvement after 10 consecutive epochs\n",
    "        early_stopping_threshold=1e-4   # Minimum absolute improvement threshold (MSE)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=ssl_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_collate_fn,  # Use the custom collate_fn\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and processor\n",
    "model_save_path = \"./AST-SSL-indep\" + str_id\n",
    "\n",
    "# Save processor in the Hugging Face supported way\n",
    "processor.save_pretrained(model_save_path)\n",
    "\n",
    "# Save model weights (since AST_SSL does not support the save_pretrained method)\n",
    "torch.save(ssl_model.state_dict(), os.path.join(model_save_path, \"model.pth\"))\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5e9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
